## Index
![grape](https://user-images.githubusercontent.com/12748752/126882595-d1f5449e-14bb-4ab3-809c-292caf0858a1.png)
![plum](https://user-images.githubusercontent.com/12748752/126882596-b9ba4645-7001-435e-9a3c-d4416a2543c1.png)

* Lazy Learner
<p align="center">
 <img src="https://github.com/user-attachments/assets/72b628c6-0abe-43e6-9506-1420aeec01e1" width=50%/>
 <br><ins><i><b>Here the `rules` are defined as `models`</b></i></ins>
</p>

K-Nearest Neighbors (KNN) is a supervised machine learning algorithm generally used for classification but can also be used for regression tasks. It works by finding the "k" closest data points (neighbors) to a given input and makes a predictions based on the majority class (for classification) or the average value (for regression). Since KNN makes no assumptions about the underlying data distribution it makes it a non-parametric and instance-based learning method.
[Link](https://www.geeksforgeeks.org/machine-learning/k-nearest-neighbours/)
